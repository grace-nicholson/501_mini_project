{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------- CSV CLEANUP PROGRAM --------------------------------------------------------------\n",
    "\n",
    "# The program opens the csv file of choice, recognises the anomalous data, removes it, and subsequently saves the new corrected data in a new file\n",
    "\n",
    "#                                 !!! MAKE SURE TO REPLACE THE PATHWAY PLACEHOLDERS WITH YOUR FILE LOCATIONS !!!\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd # Allows us to read and manipulate CSV data\n",
    "import csv \n",
    "\n",
    "def is_anomalous(cycle, start=2, period=12):\n",
    "    \"\"\"Allows us to manipulate and in this case cut out the anomalous data.\n",
    "    It considers data to be anomalous when it is equal to the starting value (2)\n",
    "    or greater going in cycles of defined size (12)\"\"\"\n",
    "\n",
    "    return cycle >= start and (cycle - start) % period == 0\n",
    "\n",
    "def clean_csv(input_csv, output_csv, cycle_column=\"cycles\"):\n",
    "    # Reads the input CSV file into a pandas DataFrame and further manipulates it\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Removes rows where the cycle value is identified as anomalous\n",
    "    df = df[~df[cycle_column].apply(is_anomalous)]\n",
    "\n",
    "    # Writes the cleaned DataFrame to a new CSV file\n",
    "    df.to_csv(\n",
    "        output_csv,\n",
    "        index=False, # Doesnt write row indicies to output\n",
    "        sep=\",\", # Uses a comma to separate columns, making it excel compatible\n",
    "        quoting=csv.QUOTE_MINIMAL # Only quotes fields when necessary\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executes the program\n",
    "    clean_csv(\n",
    "        r\"Input_Pathway\", # Locates the pathway to open the original csv data file\n",
    "        r\"Output_Pathway\" # Locates the pathway to write the new csv data file (make sure to change the title of the file if you want to keep the raw data intact)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b04e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------- JSON CLEANUP PROGRAM --------------------------------------------------------------\n",
    "\n",
    "# The program opens the json file of choice, recognises the anomalous data, removes it, and subsequently saves the new corrected data in a new file\n",
    "\n",
    "#                                   !!! MAKE SURE TO REPLACE THE PATHWAY PLACEHOLDERS WITH YOUR FILE LOCATIONS !!!\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import json # Used to analyse and process json data\n",
    "\n",
    "def is_anomalous(cycle, start=2, period=12):\n",
    "    \"\"\"Allows us to manipulate and in this case cut out the anomalous data.\n",
    "    It considers data to be anomalous when it is equal to the starting value (2)\n",
    "    or greater going in cycles of defined size (12)\"\"\"\n",
    "\n",
    "    return cycle >= start and (cycle - start) % period == 0\n",
    "\n",
    "def clean_json_ndjson(input_path, output_path, cycle_key=\"cycle\"):\n",
    "    \"\"\"Opens the file and writes data into the memory\"\"\"\n",
    "\n",
    "    with open(input_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Determines where metadata ends using brace counting, assuming that metadata is a valid object on the top\n",
    "    brace_count = 0\n",
    "    metadata_lines = []\n",
    "    ndjson_start_index = 0\n",
    "\n",
    "    # Iterates through lines to find the end of the metadata block\n",
    "    for i, line in enumerate(lines):\n",
    "        line_strip = line.strip()\n",
    "        if not line_strip:\n",
    "            continue\n",
    "        brace_count += line_strip.count('{')\n",
    "        brace_count -= line_strip.count('}')\n",
    "        metadata_lines.append(line)\n",
    "        if brace_count == 0:\n",
    "            ndjson_start_index = i + 1\n",
    "            break\n",
    "\n",
    "        # Combines metadata lines into a single string\n",
    "    metadata_str = ''.join(metadata_lines)\n",
    "    metadata = json.loads(metadata_str)\n",
    "\n",
    "    # NDJSON records\n",
    "    # Each subsequent non-empty line is expected to be a standalone JSON object\n",
    "    records = []\n",
    "    for line in lines[ndjson_start_index:]:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipped invalid JSON line: {line}\")\n",
    "\n",
    "    # Removes anomalous cycles\n",
    "    cleaned_records = [r for r in records if not is_anomalous(int(r[cycle_key]))]\n",
    "\n",
    "    # Writes data in the output\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(json.dumps(metadata, indent=4) + \"\\n\")\n",
    "        for record in cleaned_records:\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executes the program\n",
    "    clean_json_ndjson(\n",
    "        r\"Input_Pathway\", # Locates the pathway to open the original json data file\n",
    "        r\"Output_Pathway\" # Locates the pathway to write the new json data file (make sure to change the title of the file if you want to keep the raw data intact)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------ SQL PROGRAM ------------------------------------\n",
    "\n",
    "# The program reads a csv file of choice and creates an SQL library based on the data\n",
    "\n",
    "#   !!! MAKE SURE TO REPLACE THE PATHWAY PLACEHOLDERS WITH YOUR FILE LOCATIONS !!!\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "import sqlite3 # Allows SQLite database access\n",
    "import pandas as pd # Allows us to read and manipulate CSV data\n",
    "from pathlib import Path # Enables filesystem path handling\n",
    "\n",
    "\n",
    "def csv_to_sql(csv_file, db_file, table_name=\"data\"):\n",
    "    \"\"\"Converts csv into a path object\"\"\"\n",
    "    csv_file = Path(csv_file)\n",
    "\n",
    "    # Reads the csv and loads data into Panda DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Standardises the column names by making them all lower case and converting all spaces and hyphens into more compatible underscores\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "\n",
    "    # Cylces renamed to cycle for ease of use and improved readability (not a necessary step)\n",
    "    if \"cycles\" in df.columns:\n",
    "        df = df.rename(columns={\"cycles\": \"cycle\"})\n",
    "\n",
    "    # Converts selected columns to numeric types, and invalid values to NaN (\"Not a Number\"; prevents the code from crashing when comming across an invalid value)\n",
    "    numeric_colmns = [\"cycle\", \"time\", \"temp\", \"air_quality\", \"humidity\"]\n",
    "    for colmn in numeric_colmns:\n",
    "        if colmn in df.columns:\n",
    "            df[colmn] = pd.to_numeric(df[colmn], errors=\"coerce\")\n",
    "\n",
    "    # Removes the rows with invalid values, previously saved as NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Adds the variable column to enable identification of the experiment\n",
    "    df.insert(0, \"variable\", \"variable_name\") # REPLACE \"variable_name\" WITH THE EXPERIMENTAL CONDITION CHANGED\n",
    "\n",
    "    # Connects to the SQLite database and writes the DataFrame as a table\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Outputs the information about database and DataFrame table\n",
    "    print(f\"Table name: {table_name}\")\n",
    "    print(f\"Rows inserted: {len(df)}\")\n",
    "    print(\"Columns in table:\", df.columns.tolist())\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executes the program\n",
    "\n",
    "    csv_to_sql(\n",
    "        r\"Input_Pathway\", # Locates the pathway to open the original csv data file\n",
    "        r\"Output_Pathway\", # Locates the pathway to write the new db (database) file\n",
    "        table_name=\"data\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------- COMBINED SQL PROGRAM -------------------------------\n",
    "\n",
    "# The program reads multiple SQL databases and combines them into one greater one\n",
    "# This enables processing queries which spanning over multiple different experiments\n",
    "\n",
    "#  !!! MAKE SURE TO REPLACE THE PATHWAY PLACEHOLDERS WITH YOUR FILE LOCATIONS !!!\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "import sqlite3 # Allows SQLite database access\n",
    "import pandas as pd # Allows for reading SQL tables into DataFrames and writing them back\n",
    "from pathlib import Path # Enables filesystem path handling\n",
    "\n",
    "# List of SQLite databases to merge\n",
    "db_files = [\n",
    "    r\"Input_Pathway_1\",\n",
    "    r\"Input_Pathway_2\",\n",
    "    r\"Input_Pathway_3\",\n",
    "    # add more databases here\n",
    "]\n",
    "\n",
    "# Outputs the merged database\n",
    "output_db = r\"Output_Pathway\" # Make sure the name of the file differs from the original db file to prevent the data being overriden\n",
    "table_name = \"data\"\n",
    "\n",
    "# Removes output DB if it already exists clean rebuild; this is done for efficiency to ensure the table is built over each time the program is debugged\n",
    "output_path = Path(output_db)\n",
    "if output_path.exists():\n",
    "    output_path.unlink()\n",
    "\n",
    "# Connects to the output database\n",
    "out_conn = sqlite3.connect(output_db)\n",
    "\n",
    "# Loops through each input database and merges the data together\n",
    "for i, db in enumerate(db_files):\n",
    "    print(f\"Merging: {db}\")\n",
    "\n",
    "    in_conn = sqlite3.connect(db)\n",
    "\n",
    "    # Reads table into pandas\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", in_conn)\n",
    "\n",
    "    # Appends data to the output database\n",
    "    df.to_sql(\n",
    "        table_name,\n",
    "        out_conn,\n",
    "        if_exists=\"append\" if i > 0 else \"replace\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Closes the connection to the input database\n",
    "    in_conn.close()\n",
    "\n",
    "# Closes the connection to the output database\n",
    "out_conn.close()\n",
    "\n",
    "print(\"Databases merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab28e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 rows where temp < 17.5 AND humidity < 64\n",
      "\n",
      "        variable  cycle  time  temp  air_quality  humidity\n",
      "cleaning product      1 159.0 17.20         25.0      59.0\n",
      "cleaning product      3 219.0 17.31         25.0      58.0\n",
      "cleaning product      4 249.0 17.36         25.0      59.0\n",
      "cleaning product      5 279.0 17.39         25.0      58.0\n",
      "cleaning product      6 309.0 17.38         25.0      58.0\n",
      "cleaning product      7 339.0 17.40         25.0      58.0\n",
      "cleaning product      8 369.0 17.44         25.0      59.0\n",
      "cleaning product     10 429.0 17.47         26.0      58.0\n",
      "cleaning product     11 459.0 17.47         28.0      58.0\n",
      "cleaning product     12 489.0 17.49         35.0      59.0\n",
      "cleaning product     13 519.0 17.49         40.0      58.0\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------- Example for fethcing SQL queries --------------------------------------------------------------------\n",
    "\n",
    "# The code below is an example of processing queries with the created SQL library\n",
    "# The variables can be added or removed, and the conditions changed\n",
    "# For the sake of the ease of use, the most important parameters are listed in the query conditions\n",
    "# (time and air quality set to large numbers so the condition is always satisfied for the sake of the example, however its cleaner to remove them during actual data search)\n",
    "\n",
    "#                                               !!! MAKE SURE TO REPLACE THE PATHWAY PLACEHOLDERS WITH YOUR FILE LOCATIONS !!!\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Database and table\n",
    "db_file = r\"Input_Pathway\"\n",
    "table_name = \"data\"\n",
    "\n",
    "# Connects to SQLite\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "# Query: selects data points that satisfy specific conditions\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {table_name}\n",
    "WHERE CAST(time AS REAL) < 10000000\n",
    "  AND CAST(temp AS REAL) < 17.5\n",
    "  AND CAST(air_quality AS REAL) < 100000\n",
    "  AND CAST(humidity AS REAL) < 64\n",
    "\"\"\"\n",
    "\n",
    "# Fetches results into pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Prints number of rows found\n",
    "print(f\"Found {len(df)} rows where temp < 17.5 AND humidity < 64\\n\") # This line can be manually altered to display the query summary\n",
    "\n",
    "# Displays all rows without the pandas index\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Closes the connection\n",
    "conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
